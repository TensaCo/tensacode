# Planning

## Representation

- SDR's
- vector embedding
- text
- images
- custom

## Intelligence

- NN
- pretrained LLM / VLM

## Execution backend

The backend (local, hosted, remote, etc) is specified in the strategy.

## Operations

Strategies may not support all operations, but hopefully (via inheritance) they support most:

```python
encode
# text encode is no encoding for LLM-backed
#

create(description, max_depth=1, recursions=1)
  decide(...) # create bool with prompt engineered for decision
  write(...) # create str with prompt engineered for writing
  define(...) # define class

get(obj: any, n: int|str, description: str, max_depth=0) -> any
put(obj: any, val: any, description: str, max_depth=0)
sort(items: list, criteria: str) -> list
similarity(a: any, b: any) -> float
closest(item: T, choices: set[T]) -> T # find the closest match

identify_anomalies(examples: set) -> set
identify_patterns(examples: set, n=None, min=None, max=None, recognition_threshold=None) -> Pattern
identify_isomorphisms(examples: set) -> set[tuple[Pattern, set]]
group(items, n=None, min=None, max=None) # organize items into groups (number of groups is auto-detected if not specified)

predict(seq, steps=1) # predict next n elements in sequence
average(items) # produces an item with the average appearance, shape, value, etc of the inputs
combine(items) # produces an item with a semantic value equaling the combination of the inputs

_if(condition: bool, fn)
_elif(condition: bool, fn)
_else(fn)
_if(condition1: bool, fn1, condition2: bool, fn2, ..., conditionN: bool, fnN, else_fn)
unless(condition: bool, fn)
switch(condition: bool, cases: dict, default=None) # coconut is useful here
_while(condition: bool, loop)
dowhile(condition: bool, loop)

fix/debug # uses 3rd party tools to decompose program into modules of interest, tests, their expected behavior, and their actual behavior. Based on test result divergence, makes appropriate changes to code. This should be a future todo rather than main item
make_tests(type: str) # auto-generates tests that may be useful to run and runs them. Useful in an interactive computing environment
  test(...)
  benchmark(notes: str, metrics: list[str], )
analyze(items) # returns {'clusters': ..., 'trends': ..., 'statistics': ..., notes: ['blah blah blah', ...], ...}
visualize # auto-generates visualizations that may be useful to understand the object(s) (code, data, or class definitions) and runs them. Useful in an interactive computing environment

call(fn) # produced wrapped function that intelligently selects parameters when called. You should pre-curry mandatory arguments and initiate the call afterwards if you don't have any conditioning parameters to supply. Useful as a wrapper for auto-converting inputs to the proper type. EG, tc._if asks for a bool, but you can give it a string and it will convert it to a bool automatically. Also, call avoids decoding from R only to have the function re-encode it to R in the backend.

do(instructions: R) -> any? # executes a series of operations (including tensorcode operations) and returns result. The central function for tensorcode-based interpreter

# parameters are auto-encoded to R's or decoded to specific types as needed using the @tensorcode.call decorator
# all of these functions also accept a `context` and `examples` argument.
```

The model manages
- instantiating weights / training / add_loss
- check-pointing/loading/saving
- authenticating with service / deploying NN's
- context management
- implementing strategies (complex methods are only on the base class delegating primitive methods from subclasses)

There is only one program per program
Model responsibility is scoped by their context manager.

"I" is my identity. "I" includes the text that my brain evolves

```toml
[operations]


[model]
- base_model.py
- llm_model.py
- nn_model.py

```

How should training and batch inference be handled?

Also, should be able to automatically extract reward from the logging
