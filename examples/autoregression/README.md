You have many models that can explain a previous sequence of data. Activate as many of them as possible. Then sample the next value based on what is most ocngruent with the existing models. Instead of learned monolithic neural PDFs however, the models are highly structured, low-dimensional pieces of code. We would like to entertain as many models as we can fit in memory at any given time, but resource constraints will force us to optimize our model selection. The models themselves can be learned, synthesized, or supplied as ground truth.

This will be good for symbolic autoregression like music where there are many simultaneously active rhythms. It will also be good for modeling the world where there are many simultaneously active objects. It will also be good for modeling the mind where there are many simultaneously active thoughts.